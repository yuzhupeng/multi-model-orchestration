"""
æ€»ç»“ç”Ÿæˆå™¨å±æ€§æµ‹è¯•

ä½¿ç”¨ Hypothesis è¿›è¡Œå±æ€§æµ‹è¯•ï¼ŒéªŒè¯ç¼“å­˜ä¸€è‡´æ€§å’Œå…¶ä»–æ­£ç¡®æ€§å±æ€§ã€‚
Feature: multi-model-orchestration, Property 2: ç¼“å­˜ä¸€è‡´æ€§
"""
import pytest
from hypothesis import given, strategies as st
from unittest.mock import Mock, patch, MagicMock
from video_processor.summary_generator import SummaryGenerator, ModelSelector
from video_processor.cache import LRUCache, CacheKeyGenerator
from video_processor.exceptions import SummarizationError


# ç”Ÿæˆç­–ç•¥
transcript_strategy = st.text(
    alphabet=st.characters(blacklist_categories=('Cc', 'Cs')),
    min_size=10,
    max_size=1000
)

model_strategy = st.sampled_from(['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo'])

content_type_strategy = st.sampled_from(['general', 'technical', 'news', 'entertainment'])


class TestSummaryGeneratorCacheConsistency:
    """ç¼“å­˜ä¸€è‡´æ€§å±æ€§æµ‹è¯•
    
    éªŒè¯å±æ€§ 2ï¼šç¼“å­˜ä¸€è‡´æ€§
    å¯¹äºä»»ä½•å·²ç¼“å­˜çš„ç»“æœï¼Œç¬¬äºŒæ¬¡æŸ¥è¯¢åº”è¿”å›ä¸ç¬¬ä¸€æ¬¡ç›¸åŒçš„ç»“æœï¼Œè€Œä¸é‡æ–°å¤„ç†ã€‚
    **Validates: Requirements 4.3**
    """
    
    @given(transcript=transcript_strategy, model=model_strategy)
    @patch('openai.ChatCompletion.create')
    def test_cache_consistency_same_result_on_second_query(self, mock_create, transcript, model):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜ä¸€è‡´æ€§ - ç¬¬äºŒæ¬¡æŸ¥è¯¢è¿”å›ç›¸åŒç»“æœ
        
        å¯¹äºä»»ä½•è½¬å½•æ–‡æœ¬å’Œæ¨¡å‹ï¼Œç¬¬äºŒæ¬¡æŸ¥è¯¢åº”è¿”å›ä¸ç¬¬ä¸€æ¬¡ç›¸åŒçš„ç»“æœã€‚
        """
        # è®¾ç½®ç¼“å­˜
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # æ¨¡æ‹Ÿ OpenAI API å“åº”
        mock_response = MagicMock()
        mock_response.choices[0].message.content = "Generated summary"
        mock_create.return_value = mock_response
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        result1 = generator.generate(transcript, model=model)
        
        # ç¬¬äºŒæ¬¡è°ƒç”¨åº”è¯¥ä»ç¼“å­˜è¿”å›
        result2 = generator.generate(transcript, model=model)
        
        # éªŒè¯ç»“æœç›¸åŒ
        assert result1 == result2
        # éªŒè¯ API åªè¢«è°ƒç”¨ä¸€æ¬¡ï¼ˆç¬¬äºŒæ¬¡ä»ç¼“å­˜è¿”å›ï¼‰
        assert mock_create.call_count == 1
    
    @given(transcript=transcript_strategy, model=model_strategy)
    @patch('openai.ChatCompletion.create')
    def test_cache_consistency_no_reprocessing(self, mock_create, transcript, model):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜ä¸€è‡´æ€§ - ä¸é‡æ–°å¤„ç†
        
        å¯¹äºä»»ä½•å·²ç¼“å­˜çš„ç»“æœï¼Œç¬¬äºŒæ¬¡æŸ¥è¯¢ä¸åº”é‡æ–°å¤„ç†ã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        mock_response = MagicMock()
        mock_response.choices[0].message.content = "Summary"
        mock_create.return_value = mock_response
        
        # ç¬¬ä¸€æ¬¡è°ƒç”¨
        generator.generate(transcript, model=model)
        initial_call_count = mock_create.call_count
        
        # å¤šæ¬¡è°ƒç”¨
        for _ in range(5):
            generator.generate(transcript, model=model)
        
        # éªŒè¯ API è°ƒç”¨æ¬¡æ•°æ²¡æœ‰å¢åŠ 
        assert mock_create.call_count == initial_call_count
    
    @given(transcript=transcript_strategy)
    @patch('openai.ChatCompletion.create')
    def test_cache_consistency_different_models_different_results(self, mock_create, transcript):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜ä¸€è‡´æ€§ - ä¸åŒæ¨¡å‹äº§ç”Ÿä¸åŒç¼“å­˜
        
        å¯¹äºç›¸åŒçš„è½¬å½•æ–‡æœ¬ï¼Œä¸åŒçš„æ¨¡å‹åº”è¯¥äº§ç”Ÿä¸åŒçš„ç¼“å­˜æ¡ç›®ã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # ä¸ºä¸åŒæ¨¡å‹è®¾ç½®ä¸åŒçš„å“åº”
        def mock_create_side_effect(*args, **kwargs):
            model = kwargs.get('model', 'unknown')
            response = MagicMock()
            response.choices[0].message.content = f"Summary for {model}"
            return response
        
        mock_create.side_effect = mock_create_side_effect
        
        # ä½¿ç”¨ä¸åŒæ¨¡å‹ç”Ÿæˆæ€»ç»“
        result1 = generator.generate(transcript, model='gpt-3.5-turbo')
        result2 = generator.generate(transcript, model='gpt-4')
        
        # éªŒè¯ç»“æœä¸åŒ
        assert result1 != result2
        assert "gpt-3.5-turbo" in result1
        assert "gpt-4" in result2
    
    @given(transcript=transcript_strategy, model=model_strategy)
    def test_cache_consistency_with_manual_cache_operations(self, transcript, model):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜ä¸€è‡´æ€§ - æ‰‹åŠ¨ç¼“å­˜æ“ä½œ
        
        å¯¹äºä»»ä½•æ‰‹åŠ¨æ·»åŠ åˆ°ç¼“å­˜çš„ç»“æœï¼ŒæŸ¥è¯¢åº”è¿”å›ç›¸åŒçš„å€¼ã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # æ‰‹åŠ¨æ·»åŠ åˆ°ç¼“å­˜
        expected_summary = "Manually cached summary"
        key_gen = CacheKeyGenerator()
        key = key_gen.generate_summary_key(transcript, model)
        cache.set(key, expected_summary)
        
        # éªŒè¯ç¼“å­˜æ£€æŸ¥
        assert generator.is_cached(transcript, model)
        
        # éªŒè¯è·å–ç¼“å­˜çš„å€¼
        cached_value = generator.get_cached_summary(transcript, model)
        assert cached_value == expected_summary
    
    @given(transcript=transcript_strategy, model=model_strategy)
    def test_cache_consistency_delete_and_requery(self, transcript, model):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜ä¸€è‡´æ€§ - åˆ é™¤åé‡æ–°æŸ¥è¯¢
        
        å¯¹äºä»»ä½•åˆ é™¤çš„ç¼“å­˜é¡¹ï¼Œé‡æ–°æŸ¥è¯¢åº”è¿”å› Noneã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # æ‰‹åŠ¨æ·»åŠ åˆ°ç¼“å­˜
        key_gen = CacheKeyGenerator()
        key = key_gen.generate_summary_key(transcript, model)
        cache.set(key, "Cached summary")
        
        # éªŒè¯åœ¨ç¼“å­˜ä¸­
        assert generator.is_cached(transcript, model)
        
        # åˆ é™¤
        generator.delete_cached_summary(transcript, model)
        
        # éªŒè¯å·²åˆ é™¤
        assert not generator.is_cached(transcript, model)
        assert generator.get_cached_summary(transcript, model) is None


class TestModelSelectorConsistency:
    """æ¨¡å‹é€‰æ‹©å™¨ä¸€è‡´æ€§æµ‹è¯•"""
    
    @given(transcript=transcript_strategy, content_type=content_type_strategy)
    def test_model_selector_consistency(self, transcript, content_type):
        """å±æ€§æµ‹è¯•ï¼šæ¨¡å‹é€‰æ‹©ä¸€è‡´æ€§
        
        å¯¹äºç›¸åŒçš„è½¬å½•æ–‡æœ¬å’Œå†…å®¹ç±»å‹ï¼Œæ¨¡å‹é€‰æ‹©åº”è¯¥ä¸€è‡´ã€‚
        """
        selector = ModelSelector()
        
        # å¤šæ¬¡è°ƒç”¨åº”è¯¥è¿”å›ç›¸åŒçš„æ¨¡å‹
        model1 = selector.select_model(transcript, content_type=content_type)
        model2 = selector.select_model(transcript, content_type=content_type)
        model3 = selector.select_model(transcript, content_type=content_type)
        
        assert model1 == model2 == model3
    
    @given(transcript=transcript_strategy)
    def test_model_selector_returns_valid_model(self, transcript):
        """å±æ€§æµ‹è¯•ï¼šæ¨¡å‹é€‰æ‹©è¿”å›æœ‰æ•ˆæ¨¡å‹
        
        å¯¹äºä»»ä½•è½¬å½•æ–‡æœ¬ï¼Œæ¨¡å‹é€‰æ‹©åº”è¿”å›æœ‰æ•ˆçš„æ¨¡å‹åç§°ã€‚
        """
        selector = ModelSelector()
        model = selector.select_model(transcript)
        
        # éªŒè¯è¿”å›çš„æ¨¡å‹åœ¨æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ä¸­
        assert model in selector.MODELS
    
    @given(transcript=transcript_strategy)
    def test_model_selector_respects_user_preference(self, transcript):
        """å±æ€§æµ‹è¯•ï¼šæ¨¡å‹é€‰æ‹©å°Šé‡ç”¨æˆ·åå¥½
        
        å¯¹äºä»»ä½•ç”¨æˆ·åå¥½ï¼Œæ¨¡å‹é€‰æ‹©åº”è¿”å›ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹ã€‚
        """
        selector = ModelSelector()
        
        for preferred_model in ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-turbo']:
            selected_model = selector.select_model(
                transcript,
                user_preference=preferred_model
            )
            assert selected_model == preferred_model


class TestCacheKeyGeneration:
    """ç¼“å­˜é”®ç”Ÿæˆä¸€è‡´æ€§æµ‹è¯•"""
    
    @given(transcript=transcript_strategy, model=model_strategy)
    def test_cache_key_generation_consistency(self, transcript, model):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜é”®ç”Ÿæˆä¸€è‡´æ€§
        
        å¯¹äºç›¸åŒçš„è¾“å…¥ï¼Œç¼“å­˜é”®ç”Ÿæˆåº”è¯¥ä¸€è‡´ã€‚
        """
        key_gen = CacheKeyGenerator()
        
        # å¤šæ¬¡è°ƒç”¨åº”è¯¥è¿”å›ç›¸åŒçš„é”®
        key1 = key_gen.generate_summary_key(transcript, model)
        key2 = key_gen.generate_summary_key(transcript, model)
        key3 = key_gen.generate_summary_key(transcript, model)
        
        assert key1 == key2 == key3
    
    @given(transcript1=transcript_strategy, transcript2=transcript_strategy, model=model_strategy)
    def test_cache_key_generation_uniqueness(self, transcript1, transcript2, model):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜é”®ç”Ÿæˆå”¯ä¸€æ€§
        
        å¯¹äºä¸åŒçš„è¾“å…¥ï¼Œç¼“å­˜é”®åº”è¯¥ä¸åŒã€‚
        """
        key_gen = CacheKeyGenerator()
        
        # å‡è®¾ä¸¤ä¸ªä¸åŒçš„è½¬å½•æ–‡æœ¬
        if transcript1 != transcript2:
            key1 = key_gen.generate_summary_key(transcript1, model)
            key2 = key_gen.generate_summary_key(transcript2, model)
            
            # ä¸åŒçš„è¾“å…¥åº”è¯¥äº§ç”Ÿä¸åŒçš„é”®
            assert key1 != key2


class TestCacheEviction:
    """ç¼“å­˜é©±é€å±æ€§æµ‹è¯•
    
    éªŒè¯å±æ€§ 6ï¼šç¼“å­˜é©±é€
    å½“ç¼“å­˜æ»¡æ—¶ï¼Œæœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹åº”è¢«é©±é€ï¼Œæ–°é¡¹åº”è¢«æ·»åŠ ã€‚
    """
    
    @given(st.lists(transcript_strategy, min_size=5, max_size=20, unique=True))
    def test_cache_eviction_lru_order(self, transcripts):
        """å±æ€§æµ‹è¯•ï¼šLRU é©±é€é¡ºåº
        
        å½“ç¼“å­˜æ»¡æ—¶ï¼Œæœ€è¿‘æœ€å°‘ä½¿ç”¨çš„é¡¹åº”è¢«é©±é€ã€‚
        """
        cache = LRUCache(max_size=5)
        key_gen = CacheKeyGenerator()
        
        # æ·»åŠ é¡¹åˆ°ç¼“å­˜
        for i, transcript in enumerate(transcripts[:5]):
            key = key_gen.generate_summary_key(transcript, 'gpt-3.5-turbo')
            cache.set(key, f"Summary {i}")
        
        # éªŒè¯ç¼“å­˜å¤§å°
        assert cache.size() == 5
        
        # æ·»åŠ æ–°é¡¹åº”è¯¥é©±é€æœ€æ—§çš„é¡¹
        new_transcript = transcripts[5] if len(transcripts) > 5 else "new transcript"
        new_key = key_gen.generate_summary_key(new_transcript, 'gpt-3.5-turbo')
        cache.set(new_key, "New summary")
        
        # éªŒè¯ç¼“å­˜å¤§å°ä»ç„¶æ˜¯ 5
        assert cache.size() == 5
    
    @given(st.lists(transcript_strategy, min_size=10, max_size=20, unique=True))
    def test_cache_eviction_maintains_max_size(self, transcripts):
        """å±æ€§æµ‹è¯•ï¼šç¼“å­˜é©±é€ç»´æŒæœ€å¤§å¤§å°
        
        æ— è®ºæ·»åŠ å¤šå°‘é¡¹ï¼Œç¼“å­˜å¤§å°ä¸åº”è¶…è¿‡æœ€å¤§å¤§å°ã€‚
        """
        max_size = 5
        cache = LRUCache(max_size=max_size)
        key_gen = CacheKeyGenerator()
        
        # æ·»åŠ å¤šä¸ªé¡¹
        for i, transcript in enumerate(transcripts):
            key = key_gen.generate_summary_key(transcript, 'gpt-3.5-turbo')
            cache.set(key, f"Summary {i}")
            
            # éªŒè¯ç¼“å­˜å¤§å°ä¸è¶…è¿‡æœ€å¤§å¤§å°
            assert cache.size() <= max_size



class TestConcurrentProcessingIsolation:
    """å¹¶å‘å¤„ç†éš”ç¦»å±æ€§æµ‹è¯•
    
    éªŒè¯å±æ€§ 3ï¼šå¹¶å‘å¤„ç†éš”ç¦»
    å¯¹äºä»»ä½•ä¸¤ä¸ªå¹¶å‘å¤„ç†çš„è½¬å½•æ–‡æœ¬ï¼Œä¸€ä¸ªçš„å¤„ç†ä¸åº”å½±å“å¦ä¸€ä¸ªçš„ç»“æœã€‚
    **Validates: Requirements 4.4**
    """
    
    @given(
        transcripts=st.lists(
            transcript_strategy,
            min_size=2,
            max_size=10,
            unique=True
        ),
        models=st.lists(
            model_strategy,
            min_size=2,
            max_size=10
        )
    )
    @patch('openai.ChatCompletion.create')
    def test_concurrent_processing_isolation(self, mock_create, transcripts, models):
        """å±æ€§æµ‹è¯•ï¼šå¹¶å‘å¤„ç†éš”ç¦»
        
        å¯¹äºä»»ä½•ä¸¤ä¸ªå¹¶å‘å¤„ç†çš„è½¬å½•æ–‡æœ¬ï¼Œä¸€ä¸ªçš„å¤„ç†ä¸åº”å½±å“å¦ä¸€ä¸ªçš„ç»“æœã€‚
        """
        # è°ƒæ•´æ¨¡å‹åˆ—è¡¨å¤§å°ä»¥åŒ¹é…è½¬å½•æ–‡æœ¬
        if len(models) < len(transcripts):
            models = models + [models[0]] * (len(transcripts) - len(models))
        else:
            models = models[:len(transcripts)]
        
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # ä¸ºæ¯ä¸ªè½¬å½•æ–‡æœ¬è®¾ç½®ä¸åŒçš„å“åº”
        def mock_create_side_effect(*args, **kwargs):
            response = MagicMock()
            response.choices[0].message.content = "Isolated summary"
            return response
        
        mock_create.side_effect = mock_create_side_effect
        
        # å¹¶å‘ç”Ÿæˆæ€»ç»“
        results = generator.generate_concurrent(transcripts, models=models)
        
        # éªŒè¯æ‰€æœ‰ç»“æœéƒ½å·²ç”Ÿæˆ
        assert len(results) == len(transcripts)
        
        # éªŒè¯æ‰€æœ‰ç»“æœéƒ½ä¸ä¸º None
        for result in results.values():
            assert result is not None
    
    @given(
        transcripts=st.lists(
            transcript_strategy,
            min_size=2,
            max_size=5,
            unique=True
        )
    )
    @patch('openai.ChatCompletion.create')
    def test_concurrent_processing_no_cross_contamination(self, mock_create, transcripts):
        """å±æ€§æµ‹è¯•ï¼šå¹¶å‘å¤„ç†æ— äº¤å‰æ±¡æŸ“
        
        å¯¹äºä»»ä½•å¹¶å‘å¤„ç†çš„è½¬å½•æ–‡æœ¬ï¼Œç»“æœä¸åº”ç›¸äº’æ±¡æŸ“ã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # ä¸ºæ¯ä¸ªè½¬å½•æ–‡æœ¬è®¾ç½®å”¯ä¸€çš„å“åº”
        call_count = [0]
        
        def mock_create_side_effect(*args, **kwargs):
            response = MagicMock()
            response.choices[0].message.content = f"Summary {call_count[0]}"
            call_count[0] += 1
            return response
        
        mock_create.side_effect = mock_create_side_effect
        
        # å¹¶å‘ç”Ÿæˆæ€»ç»“
        results = generator.generate_concurrent(transcripts)
        
        # éªŒè¯ç»“æœæ•°é‡
        assert len(results) == len(transcripts)
        
        # éªŒè¯æ‰€æœ‰ç»“æœéƒ½ä¸ç›¸åŒï¼ˆé™¤éè½¬å½•æ–‡æœ¬ç›¸åŒï¼‰
        unique_results = set(results.values())
        # ç”±äºæˆ‘ä»¬æœ‰ä¸åŒçš„è½¬å½•æ–‡æœ¬ï¼Œåº”è¯¥æœ‰å¤šä¸ªä¸åŒçš„ç»“æœ
        assert len(unique_results) >= 1
    
    @given(
        transcripts=st.lists(
            transcript_strategy,
            min_size=2,
            max_size=5,
            unique=True
        )
    )
    @patch('openai.ChatCompletion.create')
    def test_concurrent_processing_cache_isolation(self, mock_create, transcripts):
        """å±æ€§æµ‹è¯•ï¼šå¹¶å‘å¤„ç†ç¼“å­˜éš”ç¦»
        
        å¯¹äºä»»ä½•å¹¶å‘å¤„ç†çš„è½¬å½•æ–‡æœ¬ï¼Œç¼“å­˜åº”è¯¥æ­£ç¡®éš”ç¦»æ¯ä¸ªç»“æœã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        def mock_create_side_effect(*args, **kwargs):
            response = MagicMock()
            response.choices[0].message.content = "Cached summary"
            return response
        
        mock_create.side_effect = mock_create_side_effect
        
        # ç¬¬ä¸€æ¬¡å¹¶å‘ç”Ÿæˆ
        results1 = generator.generate_concurrent(transcripts)
        
        # ç¬¬äºŒæ¬¡å¹¶å‘ç”Ÿæˆåº”è¯¥ä»ç¼“å­˜è¿”å›
        results2 = generator.generate_concurrent(transcripts)
        
        # éªŒè¯ç»“æœç›¸åŒ
        assert results1 == results2
        
        # éªŒè¯ API è°ƒç”¨æ¬¡æ•°ç­‰äºè½¬å½•æ–‡æœ¬æ•°é‡ï¼ˆç¬¬äºŒæ¬¡ä»ç¼“å­˜è¿”å›ï¼‰
        assert mock_create.call_count == len(transcripts)
    
    @given(
        transcripts=st.lists(
            transcript_strategy,
            min_size=2,
            max_size=5,
            unique=True
        )
    )
    @patch('openai.ChatCompletion.create')
    def test_concurrent_processing_error_isolation(self, mock_create, transcripts):
        """å±æ€§æµ‹è¯•ï¼šå¹¶å‘å¤„ç†é”™è¯¯éš”ç¦»
        
        å¯¹äºä»»ä½•å¹¶å‘å¤„ç†ä¸­çš„é”™è¯¯ï¼Œä¸åº”å½±å“å…¶ä»–è½¬å½•æ–‡æœ¬çš„å¤„ç†ã€‚
        """
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # ç¬¬ä¸€ä¸ªè½¬å½•æ–‡æœ¬å¤±è´¥ï¼Œå…¶ä»–æˆåŠŸ
        call_count = [0]
        
        def mock_create_side_effect(*args, **kwargs):
            if call_count[0] == 0:
                call_count[0] += 1
                raise Exception("API Error")
            response = MagicMock()
            response.choices[0].message.content = "Summary"
            call_count[0] += 1
            return response
        
        mock_create.side_effect = mock_create_side_effect
        
        # å¹¶å‘ç”Ÿæˆæ€»ç»“
        results = generator.generate_concurrent(transcripts)
        
        # éªŒè¯ç»“æœæ•°é‡
        assert len(results) == len(transcripts)
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªå¤±è´¥çš„ç»“æœ
        failed_results = [r for r in results.values() if r is None]
        assert len(failed_results) >= 1
        
        # éªŒè¯è‡³å°‘æœ‰ä¸€ä¸ªæˆåŠŸçš„ç»“æœ
        successful_results = [r for r in results.values() if r is not None]
        assert len(successful_results) >= 1


class TestSummaryGeneratorRobustness:
    """æ€»ç»“ç”Ÿæˆå™¨é²æ£’æ€§æµ‹è¯•"""
    
    @given(transcript=transcript_strategy)
    def test_generator_handles_very_long_transcripts(self, transcript):
        """å±æ€§æµ‹è¯•ï¼šå¤„ç†éå¸¸é•¿çš„è½¬å½•æ–‡æœ¬
        
        å¯¹äºä»»ä½•é•¿åº¦çš„è½¬å½•æ–‡æœ¬ï¼Œç”Ÿæˆå™¨åº”è¯¥èƒ½å¤Ÿå¤„ç†ã€‚
        """
        # åˆ›å»ºä¸€ä¸ªéå¸¸é•¿çš„è½¬å½•æ–‡æœ¬
        long_transcript = transcript * 100
        
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # éªŒè¯æ¨¡å‹é€‰æ‹©ä¸ä¼šå¤±è´¥
        model = generator.model_selector.select_model(long_transcript)
        assert model in generator.model_selector.MODELS
    
    @given(transcript=transcript_strategy)
    def test_generator_handles_special_characters(self, transcript):
        """å±æ€§æµ‹è¯•ï¼šå¤„ç†ç‰¹æ®Šå­—ç¬¦
        
        å¯¹äºåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„è½¬å½•æ–‡æœ¬ï¼Œç”Ÿæˆå™¨åº”è¯¥èƒ½å¤Ÿå¤„ç†ã€‚
        """
        # æ·»åŠ ç‰¹æ®Šå­—ç¬¦
        special_transcript = transcript + "!@#$%^&*()"
        
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # éªŒè¯ç¼“å­˜é”®ç”Ÿæˆä¸ä¼šå¤±è´¥
        key_gen = CacheKeyGenerator()
        key = key_gen.generate_summary_key(special_transcript, 'gpt-3.5-turbo')
        assert key is not None
        assert len(key) > 0
    
    @given(transcript=transcript_strategy)
    def test_generator_handles_unicode_characters(self, transcript):
        """å±æ€§æµ‹è¯•ï¼šå¤„ç† Unicode å­—ç¬¦
        
        å¯¹äºåŒ…å« Unicode å­—ç¬¦çš„è½¬å½•æ–‡æœ¬ï¼Œç”Ÿæˆå™¨åº”è¯¥èƒ½å¤Ÿå¤„ç†ã€‚
        """
        # æ·»åŠ  Unicode å­—ç¬¦
        unicode_transcript = transcript + "ä½ å¥½ä¸–ç•ŒğŸŒ"
        
        cache = LRUCache(max_size=100)
        generator = SummaryGenerator(cache=cache)
        
        # éªŒè¯ç¼“å­˜é”®ç”Ÿæˆä¸ä¼šå¤±è´¥
        key_gen = CacheKeyGenerator()
        key = key_gen.generate_summary_key(unicode_transcript, 'gpt-3.5-turbo')
        assert key is not None
        assert len(key) > 0
